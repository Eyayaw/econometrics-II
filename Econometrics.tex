\documentclass[7pt, a4paper]{article}
\usepackage[left=0.5cm,right=0.5cm,top=0.5cm,bottom=0.5cm]{geometry}
\usepackage{fontspec}
\usepackage[english]{babel}
\setmainfont[Mapping=tex-text-ms]{Resavska BG}
\usepackage{amsmath}      % AMS
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{bm}
\usepackage{econometrics}
\usepackage[parfill]{parskip}
\usepackage{mathtools} 
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\renewcommand{\thesubsection}{(\alph{subsection})}
\begin{document}
\begin{center}
\textbf{{\LARGE Econometrics II - Cheat Sheet for Dummies}}\\
Dejan Dragutinovi\'{c}\\
\end{center}

\begin{multicols*}{3}
Structural models are derived from economic theory and therefore
parameters have economic meaning.\\
Structural models: earlier simultaneous equations models, now dynamic stochastic optimization.\\
Estimation of structural models requires more assumptions but is also
more informative than reduced form parameters.\\
A model is the specification of the probability distribution for a set of
observations. Examples:\\
1. a full specification of the probability distribution of the observations.\\
2. a partial specification of distributional properties such as moments.

\textbf{Cross-section data} is collected in \textbf{a single time period} and is characterized by individual units - people, companies, countries, etc. With cross-sectional data the ordering of the data does not matter.\\
Data collected at a number of specific points in time is called time series data. Such examples include stock prices, interest rates, exchange rates as well as product prices, GDP, etc. Unlike cross-sectional data, the ordering of the data is important in \textbf{time-series data}. Each point represents the values at specific points in time. As such, time series data are typically presented in chronological order. Changing the order of the data ignores the time-dimensionality of the data.\\
\textbf{Panel data} combines cross-sectional and time series data: the same individuals (persons, firms, cities, etc.) are observed at several points in time (days, years, before and after treatment etc.).If we have the same number of time period observations for each individual, then we have a balanced panel.

FE: controls for time-invariant differences between groups. It likely has less OVB than RE because it removes any of the omitted variation due to time-constant factors. Then the remaining OVB in an FE estimation method can only come from time-varying factors.

FE: The FE estimation method has 2 identical methods: Demean each variable from its group average across time. Or, add a dummy variable for each group.

RE: As RE does not remove the time-invariant variation, you can estimate time-constant variables in an RE model. FE eliminates that variation from estimation.

RE assumptions: the time-invariant heterogeneity between groups is uncorrelated with the error term. FE does not make this assumption and thus we remove that variation. BOTH of these models assume that the error term is uncorrelated with the observable predictors to be consistently estimable

FE and RE limitations - FE controls for a lot of potential OVB, but by doing so it limits what you can estimate. For example you cannot estimate the effect of gender on something in an FE model. RE models are more relaxed in that you can do that, and they are more efficient (smaller SEs) but they risk more OVB.

RE or FE? - Generally, FE is a safer method and you should only prefer RE if you are confident that the assumptions hold. Some people think a Hausman test can help in determining which you should use.

Last note: Modeling with an FE estimation method does \textbf{NOT} eliminate all OVB. Any time-varying factors you do not adequately control for can affect your results. So while FE is safer than RE, if you care about the consistency of your coefficients, be careful with any estimation method that is not quasi-experimental like 2SLS.  

\textbf{DiD} is a tool to estimate treatment effects comparing the pre- and post-treatment differences in the outcome of a treatment and a control group. In general, we are interested in estimating the effect of a treatment Di (e.g. union status, medication, etc.) on an outcome Yi (e.g. wages, health, etc.) as in\\
$$Y_{it}=\alpha i + \lambda t + \rho D_{it} + X'_{it}\beta + \varepsilon_it$$\\
where $\alpha i$ are individual fixed effects (characteristics of individuals that do not change over time), $\lambda t$ are time fixed effects, $X_{it}$ are time-varying covariates like individuals' age, and $\varepsilon_it$ is an error term. Individuals and time are indexed by i and t, respectively. If there is a correlation between the fixed effects and $D_it$ then estimating this regression via OLS will be biased given that the fixed effects are not controlled for. This is the typical \textbf{omitted variable bias}.

To see the effect of a treatment we would like to know the difference between a person in a world in which she received the treatment and one in which she does not. Of course, only one of these is ever observable in practice. Therefore we look for people with the same pre-treatment trends in the outcome. Suppose we have two periods t=1,2
and two groups s=A,B. Then, under the assumption that the trends in the treatment and control groups would have continued the same way as before in the absence of treatment.

You can simply calculate these means by hand, i.e. obtain the mean outcome of group A in both periods and take their difference. Then obtain the mean outcome of group B in both periods and take their difference. Then take the difference in the differences and that's the treatment effect.

As stated before, DiD is a method to estimate treatment effects with non-experimental data. That's the most useful feature. DiD is also a version of fixed effects estimation. Whereas the fixed effects model assumes $E(Y_{0it}|i,t)=\alpha_i+\lambda_t$
, DiD makes a similar assumption but at the group level, $E(Y_{0it}|s,t)=\gamma_s+\lambda_t$. So the expected value of the outcome here is the sum of a group and a time effect. So what's the difference? For DiD you don't necessarily need panel data as long as your repeated cross sections are drawn from the same aggregate unit s. This makes DiD applicable to a wider array of data than the standard fixed effects models that require panel data.

DiD assumptions: 1. Stable Unit Treatment Value Assumption -
SUTVA (One and only one potential outcome observed. No peer or GE effects); 2. Exogeneity (Conditioning variables are independent of treatment.) 3. No Effect Prior to Treatment; 4. Common Trend (Control group reflects counterfactual trend of treatment group in absence of treatment.) 5. Common Support (There must exist an overlap between all group/time subsamples for all values of X).

The performance of DiD can be improved by controlling for covariates, which increases the effect of the estimated $\tau$ and makes the common time trend assumption more credible.

Solutions for standard errors in DiD: 1. cluster at the year-state level or aggregate on a year-state level; 2. block bootstrap (difficult to implement); 3. aggregate data into pre- and postintervention; 4. allow for an unrestricted covariance over time within states
Another word of caution should be applied when it comes to the treatment of standard errors. With many years of data you need to adjust the standard errors for autocorrelation. The easiest is to cluster on the individual panel identifier which allows for arbitrary correlation of the residuals among individual time series. This corrects for both autocorrelation and heteroscedasticity.

Critical assumption of OLS consistency is that x is uncorrelated with u. When there is a correlation, we use \textbf{IV} (used to control for confounding and measurement error in observational studies so that causal inferences can be made) which is correlated with x, but uncorrelated with u.
IV assumptions: 1. relevance - instrument z is correlated with x; 2. exclusion - z affects the outcome y only through x; 3. exchangeability/independence - z does not share common causes with the outcome Y

Local Average Treatment Effect - the instrumental variables treatment effect estimate is not for the entire population, but only for those who are influenced in some way by the instrument

LATE assumptions: 1. Independence; 2. Exclusion - D has to be the \textbf{only} channel through which z affects Y; 3. Monotonicity - All affected by the instrument are affected in the same direction. 

Under these assumptions the Wald estimator identifies the treatment effect for a well-defined subpopulation.

Wald estimator = Cov(outcome,instrument) /Cov(treatment,instrument); in practice, IV is often implemented in a two-stage least squares (2SLS) procedure that can be shown quite easily to be equivalent to the Wald estimator in simple cases (i.e. when not adjusting for other variables).

\textbf{Weakness} of an instrument means that instruments are weakly correlated with
the variables being instrumented. When an instrument is weak, it may also exacerbate endogeneity issues as even
small violations of exogeneity can create substantial parameter bias. It is especially important that an instrument be exogenous if an instrument is
weak since even mild endogeneity can lead to severe inconsistencies. IV may be more inconsistent than OLS when the correlation between the
instrument and the regressor is low. IV estimation always occur at the cost of precision compared to OLS. This is
intuitive since an observed variable is being predicted using other variables. The IV estimator always has larger variance than the OLS estimator unless
Cor[x,z] = 1.

IV vs. OLS: IV only exploits the variance in the control which is correlated with the instrument; IV standard errors are larger than OLS standard errors however, IV is consistent, while OLS is inconsistent; the stronger the correlation between z and x,the smaller the IV standard errors

A discrete response means that the variable to be explained, y, takes on a finite number of outcomes. Because the outcome variables are dummies, x affects
probabilities.

Logit and probit models are appropriate when attempting to model a dichotomous dependent variable, e.g. yes/no, agree/disagree, like/dislike, etc.

Several problems with this approach: 1. The regression line may lead to predictions outside the range of zero and one; 2. the functional form assumes the first unit has the same marginal effect as the tenth, which is probably not appropriate; 3. a residuals plot would quickly reveal heteroskedasticity.

What logit and probit do, in essence, is take the the linear model and feed it through a function to yield a nonlinear relationship. Whereas the linear regression predictor looks like $\hat{Y}=\alpha + \beta x$, logit and probit predictors can be written as $\hat{Y}=f(\alpha + \beta x)$.

Logit and probit differ in how they define $f(*)$. The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define $f(*)$

Both functions will take any number and rescale it to fall between 0 and 1. Hence, whatever $\alpha + \beta x$ equals, it can be transformed by the function to yield a predicted probability. Any function that would return a value between zero and one would do the trick, but there is a deeper theoretical model underpinning logit and probit that requires the function to be based on a probability distribution. The logistic and standard normal cdfs turn out to be convenient mathematically and are programmed into just about any general purpose statistical package.

Logit or probit? Both methods will yield similar (though not identical) inferences. Logit – also known as logistic regression – is more popular in health sciences like epidemiology partly because coefficients can be interpreted in terms of odds ratios. Probit models can be generalized to account for non-constant error variances in more advanced econometric settings (known as heteroskedastic probit models) and hence are used in some contexts by economists and political scientists.

A kernel density estimator smooths the intervals of the distribution of a discrete random variable according to some predetermined weighting scheme. In contrast, a histogram splits the range of a random variable x into equally spaced intervals containing the respective fraction of the sample. Choosing bandwidth is a trade-off between precision and bias, since a larger h increases bias but reduces variance and vice versa. The mean-squared error (MSE), the sum of squared bias and variance, is used
to choose bandwidth optimally, it is minimized. In contrast, the choice of kernel does not matter much as long as the optimal bandwidth is used | which varies across kernels.

A curse of dimensionality occurs because there are exponentially fewer observations within the bandwidth as the number of regressors increases. This motivates the use of semiparametric methods where some structure is applied to the regression model to make estimation feasible.

Semiparametric methods are useful because they are: 1. Subject to less restrictions than fully parametric methods; 2. Feasible in situations where a fully nonparametric analysis is infeasible.
Can retain consistency in situations where parametric estimators are inconsistent and nonparametric estimators are subject to sparseness. A semiparametric specication is therefore more robust but potentially less efficient compared to fully parametric regression. The two most commonly used semiparametric methods in econometrics are the partially linear model and the single index model.

Regression Discontinuity (RD) methods exploit precise knowledge of the rules determining assignment to treatment around a threshold value of a variable. Some thresholds are contextually arbitrary and therefore provide good natural experiments (e.g., credits, dates, quotas). Estimation can be carried out using standard regression methods or using non-parametric techniques or a combination of both. Two types: 1. Sharp RD: Related to Randomized Controlled Trial; 2. Fuzzy RD: Related to IV setting.
\end{multicols*}
\end{document}